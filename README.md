# ğŸ¦ Wiki Assignment

Wiki Assignment is a Python-based web scraper that:
- **Extracts animal names** from the [Wikipedia animal list](https://en.wikipedia.org/wiki/List_of_animal_names).
- **Visits each animal's Wikipedia page** to retrieve additional details.
- **Downloads animal images** from their respective pages.
- **Serves the collected data via a FastAPI web interface.**
- **Provides an API to refresh the data** asynchronously.

## ğŸš€ Features
âœ… **Automated Web Scraping:** Fetches animal names and images from Wikipedia.  
âœ… **Image Downloading:** Saves images locally for each animal.  
âœ… **FastAPI Web Interface:** Displays animal data in a clean table.  
âœ… **Data Refresh API:** Refreshes the dataset without restarting the server.  
âœ… **Asynchronous Processing:** Uses Python `asyncio` for concurrent scraping.  

---

## ğŸ“Œ **Installation**
### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/your-username/wiki-assignment.git
cd wiki-assignment
```
## 2ï¸âƒ£ Create and Activate a Virtual Environment
```sh
python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate     # On Windows
```
## 3ï¸âƒ£ Install Dependencies
```sh
pip install -r requirements.txt
```
## ğŸƒ Running the Project
Start the Scraper and Web Server
```sh
python main.py
```
This will:
1. Scrape animal data from Wikipedia.
2. Download images for each animal.
3. Start a FastAPI server at http://127.0.0.1:8000/.

### Access the Web Interface
- Open your browser and visit:
    ğŸ”— http://127.0.0.1:8000/
- The page will display a table of animals with:
  - Animal Names
  - Collateral Adjectives
  - Downloaded Images
  - Local Image Paths

## ğŸ”„ Refreshing Data
If new data is available, trigger a refresh **without restarting the server**.

### Method 1: Click the Refresh Button
Click the "ğŸ”„ Refresh Data" button on the homepage.

### Method 2: API Request
Send a POST request to refresh data:

```sh
curl -X POST http://127.0.0.1:8000/refresh
```

This will:
- Clear old data.
- Scrape Wikipedia again.
- Update the database and web page.

## ğŸ”— API Endpoints
### 1ï¸âƒ£ Homepage
- `GET /`
- Displays the animal data in an HTML table.

### 2ï¸âƒ£ Refresh Data
- `POST /refresh`
- Starts a background process to re-scrape Wikipedia.

### ğŸ›  Project Structure
```graphql
wiki-assignment/
â”‚â”€â”€ db/
â”‚   â”œâ”€â”€ animals_db.py          # In-memory database for storing animals
â”‚
â”‚â”€â”€ client/
â”‚   â”œâ”€â”€ http_client.py         # Handles HTTP requests
â”‚
â”‚â”€â”€ scraper/
â”‚   â”œâ”€â”€ table_scraper.py       # Scrapes animal names from Wikipedia
â”‚   â”œâ”€â”€ animal_page_scraper.py # Visits animal pages and extracts images
â”‚   â”œâ”€â”€ file_handler.py        # Downloads images and manages files
â”‚
â”‚â”€â”€ templates/
â”‚   â”œâ”€â”€ index.html             # Jinja2 template for displaying the web page
â”‚
â”‚â”€â”€ main.py                    # Runs the scraper and starts the FastAPI server
â”‚â”€â”€ requirements.txt            # Required dependencies
â”‚â”€â”€ README.md                   # Project documentation
```

## ğŸ— Future Enhancements
ğŸ”¹ Store scraped data in SQLite/PostgreSQL instead of memory.
ğŸ”¹ Add a search/filter option in the web interface.
ğŸ”¹ Implement image caching to avoid redundant downloads.

## ğŸ“ License
This project is licensed under the MIT License.

## â¤ï¸ Contributions
Contributions are welcome! Feel free to submit issues or pull requests.